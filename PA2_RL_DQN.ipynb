{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe3soqLO6ExG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be82761-6645-40f6-fc18-b119d27bc30e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "acrobot-video\tdqn_agent.py\t   models     __pycache__\tutils.py\n",
            "cartpole-video\tdqn.py\t\t   PA2.ipynb  replay_memory.py\twandb\n",
            "car-video\tfresh-sweep-1.mat  plots      Untitled0.ipynb\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/CS6700/PA2')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "o-pwyTYN6118"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!pip install gym==0.19.0"
      ],
      "metadata": {
        "id": "JbDMucok9ZiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Succesfully working\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "NyWxpZ3g7h6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ-WUjPG7mCs",
        "outputId": "1c707b2b-39c5-4450-8c2f-2cd209b16324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f9140865190>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat,savemat\n",
        "import numpy as np\n",
        "import gym\n",
        "from dqn_agent import DQNAgent\n",
        "from utils import plot_learning_curve\n",
        "from gym import wrappers\n",
        "import wandb"
      ],
      "metadata": {
        "id": "P_N0o6XB66eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_reward(pos):\n",
        "  if pos>=0.5:\n",
        "    return 10\n",
        "  else:\n",
        "    return (pos+1.2)/1.8-1"
      ],
      "metadata": {
        "id": "bNLlFOv3x5dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dqn():\n",
        "  config_defaults = {\n",
        "    \"l_size\":50,\n",
        "    \"mem_size\":100000,\n",
        "    \"lr\":5e-4,\n",
        "    \"eps_dec\":1e-5,\n",
        "    \"replace\":20,\n",
        "    \"batch_size\":64,\n",
        "    \"gamma\":0.99\n",
        "    }\n",
        "  wandb.init(config=config_defaults)\n",
        "  config = wandb.config\n",
        "  env_name = 'Acrobot-v1'\n",
        "  env = gym.make(env_name)\n",
        "  best_score = -np.inf\n",
        "  load_checkpoint = False\n",
        "  render = False\n",
        "  print(env.observation_space)\n",
        "  print(env.action_space)\n",
        "\n",
        "  agent = DQNAgent(gamma=config.gamma, epsilon=1, lr=config.lr,\n",
        "                    input_dims=(env.observation_space.shape),\n",
        "                    n_actions=env.action_space.n, mem_size=config.mem_size, eps_min=0.1,\n",
        "                    batch_size=config.batch_size, replace=config.replace, eps_dec=config.eps_dec,\n",
        "                    chkpt_dir='models/', algo='DQNAgent',\n",
        "                    env_name=env_name,fc1_dims=config.l_size,fc2_dims=round(config.l_size/2),\n",
        "                    fc3_dims=round(config.l_size/4))\n",
        "\n",
        "  if load_checkpoint:\n",
        "      agent.load_models()\n",
        "\n",
        "  fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) +'_' \\\n",
        "          + str(2000) + 'games'\n",
        "  figure_file = 'plots/' + fname + '.png'\n",
        "\n",
        "  print('name', wandb.run.name)\n",
        "\n",
        "  if render:\n",
        "    env = wrappers.Monitor(env, env_name+\"video\",\n",
        "                      video_callable=lambda episode_id: True, force=True)\n",
        "  \n",
        "  \n",
        "  s_10=[]\n",
        "  n_10=[]\n",
        "  scores, eps_history, steps_array = [], [], []\n",
        "  for i in range(2000):\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    score = 0\n",
        "    n_steps = 0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        action = agent.choose_action(observation)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        if env_name == 'MountainCar-v0': # reward scaling for car\n",
        "          reward = new_reward(observation_[0])\n",
        "        score += reward\n",
        "\n",
        "        if not load_checkpoint:\n",
        "            agent.store_transition(observation, action,\n",
        "                                  reward, observation_, done)\n",
        "            agent.learn()\n",
        "        observation = observation_\n",
        "        n_steps += 1\n",
        "    scores.append(score)\n",
        "    steps_array.append(n_steps)\n",
        "\n",
        "    avg_score = np.mean(scores[-100:])\n",
        "\n",
        "    wandb.log({\"score\":score})\n",
        "    wandb.log({\"n_steps\":n_steps})\n",
        "    wandb.log({\"mean_score\":avg_score})\n",
        "    \n",
        "    print('episode: ', i,'score: ', score,\n",
        "          ' average score %.1f' % avg_score, 'best score %.2f' % best_score,\n",
        "        'epsilon %.2f' % agent.epsilon, 'steps', n_steps)\n",
        "\n",
        "    if avg_score > best_score:\n",
        "      best_score = avg_score\n",
        "\n",
        "    eps_history.append(agent.epsilon)\n",
        "    s_10.append(scores)\n",
        "    n_10.append(steps_array)"
      ],
      "metadata": {
        "id": "YalCzViB58rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\"name\":\"acrobat_sweep\",\n",
        "                \"method\": \"bayes\",\n",
        "                \"metric\": {\n",
        "                'name': 'score',\n",
        "                'goal': 'maximize'   \n",
        "              },\n",
        "                \"parameters\":\n",
        "                {\n",
        "                    \"l_size\":{\n",
        "                        \"values\":[50,75,100]\n",
        "                    },\n",
        "                    \"mem_size\":{\n",
        "                        \"values\":[50000, 100000]\n",
        "                    },\n",
        "                    \"lr\":{\n",
        "                        \"values\":[1e-4,1e-5,8e-6]\n",
        "                    },\n",
        "                    \"eps_dec\":{\n",
        "                        \"values\":[1e-5,7e-6,5e-6]\n",
        "                    },\n",
        "                    \"replace\":{\n",
        "                        \"values\":[800,1000,1500]\n",
        "                    },\n",
        "                    \"batch_size\":{\n",
        "                        \"values\":[32,64]\n",
        "                    },\n",
        "                    \"gamma\":{\n",
        "                        \"values\":[0.9,0.99,0.999]\n",
        "                    }\n",
        "                }\n",
        "                }\n",
        "sweep_id = wandb.sweep(sweep_config,entity=\"viswa_ee\",project=\"dqn_pa2_v2\")"
      ],
      "metadata": {
        "id": "uHM1VoBdtu-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28dd5dd0-c5e8-4410-ac23-56d238c2f940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 5pczi6tm\n",
            "Sweep URL: https://wandb.ai/viswa_ee/dqn_pa2_v2/sweeps/5pczi6tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, entity=\"viswa_ee\",project=\"dqn_pa2_v2\",function=run_dqn,count=12)"
      ],
      "metadata": {
        "id": "8pxgeZp90yZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dqn():\n",
        "  config_defaults = {\n",
        "    \"l_size\":50,\n",
        "    \"mem_size\":100000,\n",
        "    \"lr\":5e-4,\n",
        "    \"eps_dec\":1e-5,\n",
        "    \"replace\":20,\n",
        "    \"batch_size\":64,\n",
        "    \"gamma\":0.99\n",
        "    }\n",
        "  wandb.init(config=config_defaults)\n",
        "  config = wandb.config\n",
        "  env_name = 'MountainCar-v0'\n",
        "  env = gym.make(env_name)\n",
        "  env._max_episode_steps = 1000\n",
        "  best_score = -np.inf\n",
        "  load_checkpoint = False\n",
        "  render = False\n",
        "  print(env.observation_space)\n",
        "  print(env.action_space)\n",
        "\n",
        "  agent = DQNAgent(gamma=config.gamma, epsilon=1, lr=config.lr,\n",
        "                    input_dims=(env.observation_space.shape),\n",
        "                    n_actions=env.action_space.n, mem_size=config.mem_size, eps_min=0.1,\n",
        "                    batch_size=config.batch_size, replace=config.replace, eps_dec=config.eps_dec,\n",
        "                    chkpt_dir='models/', algo='DQNAgent',\n",
        "                    env_name=env_name,fc1_dims=config.l_size,fc2_dims=round(config.l_size/2),\n",
        "                    fc3_dims=round(config.l_size/4))\n",
        "\n",
        "  if load_checkpoint:\n",
        "      agent.load_models()\n",
        "\n",
        "  fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) +'_' \\\n",
        "          + str(2000) + 'games'\n",
        "  figure_file = 'plots/' + fname + '.png'\n",
        "\n",
        "  print('name', wandb.run.name)\n",
        "\n",
        "  if render:\n",
        "    env = wrappers.Monitor(env, env_name+\"video\",\n",
        "                      video_callable=lambda episode_id: True, force=True)\n",
        "  \n",
        "  \n",
        "  s_10=[]\n",
        "  n_10=[]\n",
        "  scores, eps_history, steps_array = [], [], []\n",
        "  for i in range(10000):\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    score = 0\n",
        "    n_steps = 0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        action = 2#agent.choose_action(observation)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        # if env_name == 'MountainCar-v0': # reward scaling for car\n",
        "        #   reward = new_reward(observation_[0])\n",
        "        # if reward == 2:\n",
        "        #   print('solved env...')\n",
        "        score += reward\n",
        "\n",
        "        if not load_checkpoint:\n",
        "            agent.store_transition(observation, action,\n",
        "                                  reward, observation_, done)\n",
        "            agent.learn()\n",
        "        observation = observation_\n",
        "        n_steps += 1\n",
        "    scores.append(score)\n",
        "    steps_array.append(n_steps)\n",
        "\n",
        "    avg_score = np.mean(scores[-100:])\n",
        "\n",
        "    wandb.log({\"score\":score})\n",
        "    wandb.log({\"n_steps\":n_steps})\n",
        "    wandb.log({\"mean_score\":avg_score})\n",
        "    \n",
        "    print('episode: ', i,'score: ', score,\n",
        "          ' average score %.1f' % avg_score, 'best score %.2f' % best_score,\n",
        "        'epsilon %.2f' % agent.epsilon, 'steps', n_steps)\n",
        "\n",
        "    if avg_score > best_score:\n",
        "      best_score = avg_score\n",
        "\n",
        "    eps_history.append(agent.epsilon)\n",
        "    s_10.append(scores)\n",
        "    n_10.append(steps_array)"
      ],
      "metadata": {
        "id": "nDDFZe-SNVW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\"name\":\"mountain_car_sweep\",\n",
        "                \"method\": \"bayes\",\n",
        "                \"metric\": {\n",
        "                'name': 'mean_score',\n",
        "                'goal': 'maximize'   \n",
        "              },\n",
        "                \"parameters\":\n",
        "                {\n",
        "                    \"l_size\":{\n",
        "                        \"values\":[50,75,100]\n",
        "                    },\n",
        "                    \"mem_size\":{\n",
        "                        \"values\":[50000, 100000]\n",
        "                    },\n",
        "                    \"lr\":{\n",
        "                        \"values\":[1e-4,1e-5,8e-6]\n",
        "                    },\n",
        "                    \"eps_dec\":{\n",
        "                        \"values\":[1e-5,7e-6,5e-6]\n",
        "                    },\n",
        "                    \"replace\":{\n",
        "                        \"values\":[800,1000,1500]\n",
        "                    },\n",
        "                    \"batch_size\":{\n",
        "                        \"values\":[32,64]\n",
        "                    },\n",
        "                    \"gamma\":{\n",
        "                        \"values\":[0.9,0.99,0.999]\n",
        "                    }\n",
        "                }\n",
        "                }\n",
        "sweep_id = wandb.sweep(sweep_config,entity=\"viswa_ee\",project=\"dqn_pa2_v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "5ExWVHdANg7x",
        "outputId": "0cd792fe-058f-4ea0-f19f-d2bcb2dd55cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: qd6at36w\n",
            "Sweep URL: https://wandb.ai/viswa_ee/dqn_pa2_v2/sweeps/qd6at36w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, entity=\"viswa_ee\",project=\"dqn_pa2_v2\",function=run_dqn,count=12)"
      ],
      "metadata": {
        "id": "RScuw7-bNluj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3d03d33c-acc5-452d-93e7-25a9467d4c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jeh7l911 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teps_dec: 1e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tl_size: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmem_size: 50000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \treplace: 1000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mviswa_ee\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/gdrive/MyDrive/CS6700/PA2/wandb/run-20220323_143854-jeh7l911</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/viswa_ee/dqn_pa2_v2/runs/jeh7l911\" target=\"_blank\">treasured-sweep-1</a></strong> to <a href=\"https://wandb.ai/viswa_ee/dqn_pa2_v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/viswa_ee/dqn_pa2_v2/sweeps/qd6at36w\" target=\"_blank\">https://wandb.ai/viswa_ee/dqn_pa2_v2/sweeps/qd6at36w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
            "Discrete(3)\n",
            "name treasured-sweep-1\n",
            "episode:  0 score:  -1000.0  average score -1000.0 best score -inf epsilon 0.99 steps 1000\n",
            "episode:  1 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.98 steps 1000\n",
            "episode:  2 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.97 steps 1000\n",
            "episode:  3 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.96 steps 1000\n",
            "episode:  4 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.95 steps 1000\n",
            "episode:  5 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.94 steps 1000\n",
            "episode:  6 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.93 steps 1000\n",
            "episode:  7 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.92 steps 1000\n",
            "episode:  8 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.91 steps 1000\n",
            "episode:  9 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.90 steps 1000\n",
            "episode:  10 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.89 steps 1000\n",
            "episode:  11 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.88 steps 1000\n",
            "episode:  12 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.87 steps 1000\n",
            "episode:  13 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.86 steps 1000\n",
            "episode:  14 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.85 steps 1000\n",
            "episode:  15 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.84 steps 1000\n",
            "episode:  16 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.83 steps 1000\n",
            "episode:  17 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.82 steps 1000\n",
            "episode:  18 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.81 steps 1000\n",
            "episode:  19 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.80 steps 1000\n",
            "episode:  20 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.79 steps 1000\n",
            "episode:  21 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.78 steps 1000\n",
            "episode:  22 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.77 steps 1000\n",
            "episode:  23 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.76 steps 1000\n",
            "episode:  24 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.75 steps 1000\n",
            "episode:  25 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.74 steps 1000\n",
            "episode:  26 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.73 steps 1000\n",
            "episode:  27 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.72 steps 1000\n",
            "episode:  28 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.71 steps 1000\n",
            "episode:  29 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.70 steps 1000\n",
            "episode:  30 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.69 steps 1000\n",
            "episode:  31 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.68 steps 1000\n",
            "episode:  32 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.67 steps 1000\n",
            "episode:  33 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.66 steps 1000\n",
            "episode:  34 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.65 steps 1000\n",
            "episode:  35 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.64 steps 1000\n",
            "episode:  36 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.63 steps 1000\n",
            "episode:  37 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.62 steps 1000\n",
            "episode:  38 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.61 steps 1000\n",
            "episode:  39 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.60 steps 1000\n",
            "episode:  40 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.59 steps 1000\n",
            "episode:  41 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.58 steps 1000\n",
            "episode:  42 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.57 steps 1000\n",
            "episode:  43 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.56 steps 1000\n",
            "episode:  44 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.55 steps 1000\n",
            "episode:  45 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.54 steps 1000\n",
            "episode:  46 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.53 steps 1000\n",
            "episode:  47 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.52 steps 1000\n",
            "episode:  48 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.51 steps 1000\n",
            "episode:  49 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.50 steps 1000\n",
            "episode:  50 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.49 steps 1000\n",
            "episode:  51 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.48 steps 1000\n",
            "episode:  52 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.47 steps 1000\n",
            "episode:  53 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.46 steps 1000\n",
            "episode:  54 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.45 steps 1000\n",
            "episode:  55 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.44 steps 1000\n",
            "episode:  56 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.43 steps 1000\n",
            "episode:  57 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.42 steps 1000\n",
            "episode:  58 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.41 steps 1000\n",
            "episode:  59 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.40 steps 1000\n",
            "episode:  60 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.39 steps 1000\n",
            "episode:  61 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.38 steps 1000\n",
            "episode:  62 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.37 steps 1000\n",
            "episode:  63 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.36 steps 1000\n",
            "episode:  64 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.35 steps 1000\n",
            "episode:  65 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.34 steps 1000\n",
            "episode:  66 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.33 steps 1000\n",
            "episode:  67 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.32 steps 1000\n",
            "episode:  68 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.31 steps 1000\n",
            "episode:  69 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.30 steps 1000\n",
            "episode:  70 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.29 steps 1000\n",
            "episode:  71 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.28 steps 1000\n",
            "episode:  72 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.27 steps 1000\n",
            "episode:  73 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.26 steps 1000\n",
            "episode:  74 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.25 steps 1000\n",
            "episode:  75 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.24 steps 1000\n",
            "episode:  76 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.23 steps 1000\n",
            "episode:  77 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.22 steps 1000\n",
            "episode:  78 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.21 steps 1000\n",
            "episode:  79 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.20 steps 1000\n",
            "episode:  80 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.19 steps 1000\n",
            "episode:  81 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.18 steps 1000\n",
            "episode:  82 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.17 steps 1000\n",
            "episode:  83 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.16 steps 1000\n",
            "episode:  84 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.15 steps 1000\n",
            "episode:  85 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.14 steps 1000\n",
            "episode:  86 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.13 steps 1000\n",
            "episode:  87 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.12 steps 1000\n",
            "episode:  88 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.11 steps 1000\n",
            "episode:  89 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  90 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  91 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  92 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  93 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  94 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  95 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  96 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  97 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  98 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  99 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  100 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  101 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  102 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  103 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  104 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  105 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  106 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  107 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  108 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  109 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  110 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  111 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  112 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  113 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  114 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  115 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  116 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  117 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  118 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  119 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  120 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  121 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  122 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  123 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  124 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  125 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  126 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  127 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  128 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  129 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  130 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  131 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  132 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  133 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  134 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  135 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  136 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  137 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  138 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  139 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  140 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  141 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  142 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  143 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  144 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  145 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  146 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  147 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  148 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  149 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  150 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  151 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  152 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  153 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  154 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  155 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  156 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  157 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  158 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  159 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  160 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  161 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  162 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  163 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  164 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  165 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  166 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  167 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  168 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  169 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  170 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  171 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  172 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  173 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  174 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  175 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  176 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  177 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  178 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  179 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  180 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  181 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  182 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  183 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  184 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  185 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  186 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  187 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  188 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  189 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  190 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  191 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  192 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  193 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  194 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  195 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  196 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  197 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  198 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  199 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  200 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  201 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  202 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  203 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  204 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  205 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  206 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  207 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  208 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  209 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  210 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  211 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  212 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  213 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  214 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  215 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  216 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  217 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  218 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  219 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  220 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  221 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  222 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  223 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  224 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  225 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  226 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  227 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  228 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  229 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  230 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  231 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  232 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  233 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  234 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  235 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  236 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  237 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  238 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  239 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  240 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  241 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  242 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  243 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  244 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  245 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  246 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  247 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  248 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  249 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  250 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  251 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  252 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  253 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  254 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  255 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  256 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  257 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  258 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  259 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  260 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  261 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  262 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  263 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  264 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  265 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  266 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  267 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  268 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  269 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  270 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  271 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  272 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  273 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  274 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  275 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  276 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  277 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  278 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  279 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  280 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  281 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  282 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  283 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  284 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  285 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  286 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  287 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  288 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  289 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  290 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  291 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  292 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  293 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  294 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  295 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  296 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  297 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  298 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  299 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  300 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  301 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  302 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  303 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  304 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  305 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  306 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  307 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  308 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  309 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  310 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  311 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  312 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  313 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  314 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  315 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  316 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  317 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  318 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  319 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  320 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  321 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  322 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  323 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  324 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  325 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  326 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  327 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  328 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  329 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n",
            "episode:  330 score:  -1000.0  average score -1000.0 best score -1000.00 epsilon 0.10 steps 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        " \n",
        "def show_video(video_path, video_width = 600):   \n",
        "  video_file = open(video_path, \"r+b\").read() \n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
      ],
      "metadata": {
        "id": "IGw5Hg2yjnzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_video('./acrobot-video/openaigym.video.0.872.video000019.mp4')"
      ],
      "metadata": {
        "id": "Qpixq4M9kWUz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}