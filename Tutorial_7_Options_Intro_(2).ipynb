{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn7PKu9r0asK"
      },
      "source": [
        "#Tutorial 5 - Options Intro\n",
        "\n",
        "Please complete this tutorial to get an overview of options and an implementation of SMDP Q-Learning and Intra-Option Q-Learning.\n",
        "\n",
        "\n",
        "### References:\n",
        "\n",
        " [Recent Advances in Hierarchical Reinforcement\n",
        "Learning](https://people.cs.umass.edu/~mahadeva/papers/hrl.pdf) is a strong recommendation for topics in HRL that was covered in class. Watch Prof. Ravi's lectures on moodle or nptel for further understanding the core concepts. Contact the TAs for further resources if needed. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_DODRgW_ZKS",
        "tags": []
      },
      "outputs": [],
      "source": [
        "'''\n",
        "A bunch of imports, you don't have to worry about these\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNA5kiH_esJ",
        "outputId": "626ad5c9-8608-4ccb-8b05-f72ca4af8d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36\n",
            "Number of states: 48\n",
            "Number of actions that an agent can take: 4\n",
            "Action taken: down\n",
            "Transition probability: {'prob': 1.0}\n",
            "Next state: 36\n",
            "Reward recieved: -1\n",
            "Terminal state: False\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "The environment used here is extremely similar to the openai gym ones.\n",
        "At first glance it might look slightly different. \n",
        "The usual commands we use for our experiments are added to this cell to aid you\n",
        "work using this environment.\n",
        "'''\n",
        "\n",
        "#Setting up the environment\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
        "env = CliffWalkingEnv()\n",
        "\n",
        "env.reset()\n",
        "\n",
        "#Current State\n",
        "print(env.s)\n",
        "\n",
        "# 4x12 grid = 48 states\n",
        "print (\"Number of states:\", env.nS)\n",
        "\n",
        "# Primitive Actions\n",
        "action = [\"up\", \"right\", \"down\", \"left\"]\n",
        "#correspond to [0,1,2,3] that's actually passed to the environment\n",
        "\n",
        "# either go left, up, down or right\n",
        "print (\"Number of actions that an agent can take:\", env.nA)\n",
        "\n",
        "# Example Transitions\n",
        "rnd_action = random.randint(0, 3)\n",
        "print (\"Action taken:\", action[rnd_action])\n",
        "next_state, reward, is_terminal, t_prob = env.step(rnd_action)\n",
        "print (\"Transition probability:\", t_prob)\n",
        "print (\"Next state:\", next_state)\n",
        "print (\"Reward recieved:\", reward)\n",
        "print (\"Terminal state:\", is_terminal)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuaOxavDXus"
      },
      "source": [
        "#### Options\n",
        "We custom define very simple options here. They might not be the logical options for this settings deliberately chosen to visualise the Q Table better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4MRC1p2DZbp",
        "outputId": "c481bbc8-2a7f-4362-c7e8-007ca5f3f0c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nNow the new action space will contain\\nPrimitive Actions: [\"up\", \"right\", \"down\", \"left\"]\\nOptions: [\"Away\",\"Close\"]\\nTotal Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\\nCorresponding to [0,1,2,3,4,5]\\n'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We are defining two more options here\n",
        "# Option 1 [\"Away\"] - > Away from Cliff (ie keep going up)\n",
        "# Option 2 [\"Close\"] - > Close to Cliff (ie keep going down) \n",
        "\n",
        "def Away(env,state):\n",
        "    \n",
        "    optdone = False\n",
        "    optact = 0\n",
        "    \n",
        "    if (int(state/12) == 0):\n",
        "        optdone = True\n",
        "    \n",
        "    return [optact,optdone]\n",
        "    \n",
        "def Close(env,state):\n",
        "    \n",
        "    optdone = False\n",
        "    optact = 2\n",
        "    \n",
        "    if (int(state/12) == 2) or (int(state/12)==3):\n",
        "        optdone = True\n",
        "    \n",
        "    return [optact,optdone]\n",
        "    \n",
        "    \n",
        "'''\n",
        "Now the new action space will contain\n",
        "Primitive Actions: [\"up\", \"right\", \"down\", \"left\"]\n",
        "Options: [\"Away\",\"Close\"]\n",
        "Total Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\n",
        "Corresponding to [0,1,2,3,4,5]\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmv5c0XoK8GA"
      },
      "source": [
        "# Task 1\n",
        "Complete the code cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh_oghc7Ledh"
      },
      "outputs": [],
      "source": [
        "#Q-Table: (States x Actions) === (env.ns(48) x total actions(6))\n",
        "q_values_SMDP = np.zeros((48,6))\n",
        "\n",
        "#Update_Frequency Data structure? Check TODO 4\n",
        "option_freq_SMDP = np.zeros(q_values_SMDP.shape)\n",
        "\n",
        "# TODO: epsilon-greedy action selection function\n",
        "def egreedy_policy(q_values,state,epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        # Choose a random action\n",
        "        action = np.random.randint(q_values.shape[1])\n",
        "    else:\n",
        "        # Choose the best action according to the Q-values\n",
        "        action = np.argmax(q_values[state])\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8VJYkqoLqlO"
      },
      "source": [
        "# Task 2\n",
        "Below is an incomplete code cell with the flow of SMDP Q-Learning. Complete the cell and train the agent using SMDP Q-Learning algorithm.\n",
        "Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok_5eQM7OCTj",
        "outputId": "bd31544e-81fc-4fd4-b94f-13b3e0dc828c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPISODE: 999"
          ]
        }
      ],
      "source": [
        "#### SMDP Q-Learning \n",
        "\n",
        "# Add parameters you might need here\n",
        "gamma = 0.9\n",
        "alpha = 0.4\n",
        "\n",
        "# Iterate over 1000 episodes\n",
        "for _ in range(1000):\n",
        "    state = env.reset()    \n",
        "    done = False\n",
        "    print(f\"\\rEPISODE: {_}\", end = \"\")\n",
        "    # While episode is not over\n",
        "    while not done:\n",
        "        _state = state\n",
        "        \n",
        "        # Choose action        \n",
        "        action = egreedy_policy(q_values_SMDP, state, epsilon=0.1)\n",
        "        \n",
        "        # Checking if primitive action\n",
        "        if action < 4:\n",
        "            # Perform regular Q-Learning update for state-action pair\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            q_values_SMDP[state, action] += alpha * (reward + gamma * np.max(q_values_SMDP[next_state]) - q_values_SMDP[state, action])\n",
        "            state = next_state\n",
        "            option_freq_SMDP[state][action] += 1\n",
        "        \n",
        "        # Checking if action chosen is an option\n",
        "        reward_bar = 0\n",
        "        tau = 0\n",
        "        if action == 4: # action => Away option\n",
        "            \n",
        "            optdone = False\n",
        "            while (optdone == False):\n",
        "                \n",
        "                # Think about what this function might do?\n",
        "                optact,optdone = Away(env,state) \n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "                \n",
        "                # Is this formulation right? What is this term?\n",
        "                reward_bar = gamma*reward_bar + reward\n",
        "                tau += 1\n",
        "            \n",
        "                # Complete SMDP Q-Learning Update\n",
        "                \n",
        "                # Remember SMDP Updates. When & What do you update? \n",
        "                if optdone or done:\n",
        "                    q_values_SMDP[_state, action] += alpha * (reward_bar + (gamma**tau) * np.max(q_values_SMDP[next_state]) - q_values_SMDP[_state, action])\n",
        "                    option_freq_SMDP[_state][action] += 1\n",
        "                \n",
        "                state = next_state\n",
        "           \n",
        "        if action == 5: # action => Close option\\\n",
        "            optdone = False\n",
        "            while (optdone == False):\n",
        "                \n",
        "                # Think about what this function might do?\n",
        "                optact,optdone = Close(env,state) \n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "                \n",
        "                # Is this formulation right? What is this term?\n",
        "                reward_bar = gamma*reward_bar + reward\n",
        "                tau += 1\n",
        "            \n",
        "                # Complete SMDP Q-Learning Update\n",
        "                \n",
        "                # Remember SMDP Updates. When & What do you update? \n",
        "                if optdone or done:\n",
        "                    q_values_SMDP[_state, action] += alpha * (reward_bar + (gamma**tau) * np.max(q_values_SMDP[next_state]) - q_values_SMDP[_state, action])\n",
        "                    option_freq_SMDP[_state][action] += 1\n",
        "                \n",
        "                state = next_state\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9fFNkp99Zre",
        "outputId": "ab3bfc18-aa09-418d-c7f9-828a52155d44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  -7.45813417, -106.71228012,   -7.71232074,   -7.71232063,\n",
              "         -8.49547387,   -7.71231566])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q_values_SMDP[36]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SQFbRCHWQyO"
      },
      "source": [
        "# Task 3\n",
        "Using the same options and the SMDP code, implement Intra Option Q-Learning (In the code cell below). You *might not* always have to search through options to find the options with similar policies, think about it. Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5lb6iuW9Zrf"
      },
      "outputs": [],
      "source": [
        "#Q-Table: (States x Actions) === (env.ns(48) x total actions(6))\n",
        "q_values_IO = np.zeros((48,6))\n",
        "\n",
        "#Update_Frequency Data structure? Check TODO 4\n",
        "option_freq_IO = np.zeros(q_values_SMDP.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6A2TdUHWVUN"
      },
      "outputs": [],
      "source": [
        "#### Intra-Option Q-Learning \n",
        "\n",
        "# Add parameters you might need here\n",
        "gamma = 0.9\n",
        "alpha = 0.1\n",
        "\n",
        "# Iterate over 1000 episodes\n",
        "for _ in range(1000):\n",
        "    state = env.reset()    \n",
        "    done = False\n",
        "\n",
        "    # While episode is not over\n",
        "    while not done:\n",
        "        \n",
        "        # Choose action        \n",
        "        action = egreedy_policy(q_values_SMDP, state, epsilon=0.1)\n",
        "        \n",
        "        # Checking if primitive action\n",
        "        if action < 4:\n",
        "            # Perform regular Q-Learning update for state-action pair\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            q_values_IO[state, action] += alpha * (reward + gamma * np.max(q_values_IO[next_state]) - q_values_IO[state, action])\n",
        "            state = next_state\n",
        "            option_freq_IO[state][action] += 1\n",
        "        \n",
        "        # Checking if action chosen is an option\n",
        "        reward_bar = 0\n",
        "        tau = 0\n",
        "        if action == 4: # action => Away option\n",
        "            \n",
        "            optdone = False\n",
        "            while (optdone == False and not done):\n",
        "                \n",
        "                # Think about what this function might do?\n",
        "                optact,optdone = Away(env,state) \n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "                \n",
        "                # Is this formulation right? What is this term?\n",
        "                reward_bar = gamma*reward_bar + reward\n",
        "                tau += 1\n",
        "            \n",
        "                # Complete IO Q Learning Update\n",
        "                \n",
        "                # Remember SMDP Updates. When & What do you update? \n",
        "                q_values_IO[state, optact] += alpha * (reward + gamma * np.max(q_values_IO[next_state]) - q_values_IO[state, optact])\n",
        "                option_freq_IO[state][optact] += 1\n",
        "\n",
        "                # options qvalues\n",
        "                q_values_IO[state][action] += alpha * (reward + gamma * ( (1 - optdone)*q_values_IO[next_state][action] + optdone * np.max(q_values_IO[next_state])) - q_values_IO[state][action] )\n",
        "                option_freq_IO[state][action] += 1\n",
        "                # since the policies of both the options never suggest same action we need not update qvalues and freq for other option\n",
        "                state = next_state\n",
        "           \n",
        "        if action == 5: # action => Close option\\\n",
        "            optdone = False\n",
        "            while (optdone == False and not done):\n",
        "                \n",
        "                # Think about what this function might do?\n",
        "                optact,optdone = Close(env,state) \n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "                \n",
        "                # Is this formulation right? What is this term?\n",
        "                reward_bar = gamma*reward_bar + reward\n",
        "                tau += 1\n",
        "            \n",
        "                # Complete SMDP Q-Learning Update\n",
        "                \n",
        "                # Remember SMDP Updates. When & What do you update? \n",
        "                q_values_IO[state, optact] += alpha * (reward + gamma * np.max(q_values_IO[next_state]) - q_values_IO[state, optact])\n",
        "                option_freq_IO[state][optact] += 1\n",
        "                \n",
        "                # options q_values\n",
        "                q_values_IO[state][action] += alpha * (reward + gamma * ( (1 - optdone)*q_values_IO[next_state][action] + optdone * np.max(q_values_IO[next_state])) - q_values_IO[state][action] )\n",
        "                option_freq_IO[state][action] += 1\n",
        "                # since the policies of both the options never suggest same action we need not update qvalues and freq for other option\n",
        "                state = next_state\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzUgcwL-VfkO"
      },
      "source": [
        "# Task 4\n",
        "Compare the two Q-Tables and Update Frequencies and provide comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8mZE74_Vhmg"
      },
      "outputs": [],
      "source": [
        "# Use this cell for Task 4 Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SemE13ORV04n"
      },
      "source": [
        "Use this text cell for your comments - Task 4\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5LBh6_lOVBdN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}